{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement: How have hits changed over the years?\n",
    "\n",
    "**The goal of this project is to examine how hits have changed over the years in terms of a song's specific features and its lyrics. We will focus on how hits have changed since 2008, but will specifically compare changes from the years 2008 to 2013 to 2018.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using Spotify's API and Web Scraping methods to gather all of our data\n",
    "- Scraping from Billboard's Year-End Hot 100 Songs will provide us with songs to use as a proxy for hit songs.\n",
    "- Scraping from SongFacts will provide us with other songs that are not on the Year-End Hot 100 list for each year, but were released in the same year as each Hot 100 list was compiled.\n",
    "- Genius' API will provide us with song lyrics for the tracks we will use in our model.\n",
    "- Spotify's API will provide us with important audio features that we need for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy                     # Spotify's API packagelibrary\n",
    "import spotipy.oauth2 as oauth2    # Spotify's authorization sublibrary\n",
    "import lyricsgenius as genius      # Genius' API package library\n",
    "import json                        \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's scrape Billboard's Year-End Hot 100 Songs using BeautifulSoup\n",
    "- We will be scraping Year-End charts from 2008, 2013 and 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating URLs for Year-End Hot 100 Songs for years 2008, 2013 and 2018\n",
    "url_billboard_2008 = 'https://www.billboard.com/charts/year-end/2008/hot-100-songs'\n",
    "url__billboard_2013 = 'https://www.billboard.com/charts/year-end/2013/hot-100-songs'\n",
    "url_billboard_2018 = 'https://www.billboard.com/charts/year-end/2018/hot-100-songs'\n",
    "\n",
    "# Create a master list of all URLs \n",
    "billboard_master_url_list = [url_billboard_2008, url__billboard_2013, url_billboard_2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes a URL and scrapes all track names from that specific year's Year-End Hot 100 list\n",
    "def get_track_names(url):\n",
    "    res = requests.get(url)                       # Instantiate a get request for the url\n",
    "    soup = BeautifulSoup(res.content, 'lxml')     # Instantiate a soup object\n",
    "    \n",
    "    soup_track_names = soup.find_all('div', {'class': 'ye-chart-item__title'})     # Soup object query for track names\n",
    "    track_names = []                                                               # Create an empty list to fill with track names\n",
    "    \n",
    "    # Create a for loop to iterate through each json row returned from the query\n",
    "    for i in range(len(soup_track_names)):              \n",
    "        stripped_title = soup_track_names[i].text.strip('\\n')          # Strip '\\n' from the track names\n",
    "        track_names.append(stripped_title)                             # Append the list with stripped title names\n",
    "        \n",
    "    return track_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes a URL and scrapes all artist names from that specific year's Year-End Hot 100 list\n",
    "def get_artist_names(url):\n",
    "    res = requests.get(url)                       # Instantiate a get request for the url\n",
    "    soup = BeautifulSoup(res.content, 'lxml')     # Instantiate a soup object\n",
    "    \n",
    "    soup_artist_names = soup.find_all('div', {'class': 'ye-chart-item__artist'})   # Soup object query for artist names\n",
    "    artist_names = []                                                              # Create an empty list to fill with artist names\n",
    "    \n",
    "    # Create a for loop to iterate through each json row returned from the query\n",
    "    for i in range(len(soup_artist_names)):              \n",
    "        stripped_artist = soup_artist_names[i].text.strip('\\n')        # Strip '\\n' from the artist names\n",
    "        artist_names.append(stripped_artist)                           # Append the list with stripped artist names\n",
    "        \n",
    "    return artist_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables for all hit songs and respective artists for years 2008, 2013 and 2018\n",
    "hit_songs_08 = get_track_names(url_billboard_2008)\n",
    "hit_artists_08 = get_artist_names(url_billboard_2008)\n",
    "\n",
    "hit_songs_13 = get_track_names(url__billboard_2013)\n",
    "hit_artists_13 = get_artist_names(url__billboard_2013)\n",
    "\n",
    "hit_songs_18 = get_track_names(url_billboard_2018)\n",
    "hit_artists_18 = get_artist_names(url_billboard_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, lets scrape songs that were not on the Year-End Hot 100 list from SongFacts\n",
    "- SongFacts is a website that provides expansive lists of all songs that were released in a given year in the US. Whether the site has successfully captured every single song release in the specified year is not 100% certain, but for the purposes of this project, the quantity of songs available is what we are going for. Similar to the steps taken above for scraping Billboard, we will proceed with a similar process for SongFacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating URLs for SongFacts browse pages for years 2008, 2013 and 2018\n",
    "url_songfacts_2008 = \"https://www.songfacts.com/browse/years/2008\"\n",
    "url_songfacts_2013 = \"https://www.songfacts.com/browse/years/2013\"\n",
    "url_songfacts_2018 = \"https://www.songfacts.com/browse/years/2018\"\n",
    "\n",
    "# Create a master list of all URLs\n",
    "songfacts_master_url_list = [url_songfacts_2008, url_songfacts_2013, url_songfacts_2018]\n",
    "\n",
    "# Instantiating a user agent for SongFacts web scrape\n",
    "headers = {'User-agent': 'danhyunkim'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will create separate functions to scrape all song and artist names for each individual year. We tried creating one function to do this for each year, but ran into issues when trying to scrape multiple pages for each year.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate empty lists to fill with all song and artist names from 2008\n",
    "sf_songs_2008 = []\n",
    "sf_artists_2008 = []\n",
    "\n",
    "# Define the scraping function for 2008\n",
    "# Inputs are the url, a song list and an artist list\n",
    "def get_sf_songs_2008(url, song_list, artist_list):\n",
    "    res_url = requests.get(url, headers=headers)         # Create a get request for the url\n",
    "    sf_soup = BeautifulSoup(res_url.content, 'lxml')     # Create a soup object for the url\n",
    "\n",
    "    # This range is specific to the json data that the below query delivers\n",
    "    # Each page has 100 songs with their respective artists which starts at index 39 and ends at index 138\n",
    "    for i in range(39, 139):\n",
    "        \n",
    "        # Query for getting json data for song and artist names\n",
    "        # The query will return a list in this format: ['song name - artist name']\n",
    "        # The code will replace the hyphen with a comma and split on the comma to create two separate strings as such: ['song name', 'artist name']\n",
    "        song_and_artist = sf_soup.find_all('li')[i].text.replace('-', ',').split(',')\n",
    "        \n",
    "        # Append the first item of each list created to empty song list\n",
    "        sf_songs_2008.append(song_and_artist[0])              \n",
    "        \n",
    "        # Append the second item of each list created to empty artist list and replace spaces with empty string\n",
    "        sf_artists_2008.append(song_and_artist[1].replace(' ', ''))        \n",
    "        \n",
    "        # Break this for loop here because a new for loop is necessary for the remaining pages of songs\n",
    "        break\n",
    "        \n",
    "    # New for loop to scrape songs and artists from page 2 through 21\n",
    "    # This range is specific for the year 2008\n",
    "    for i in range(2,22):\n",
    "        # Base url that will be used to create the URL for each page\n",
    "        url_next_page = 'https://www.songfacts.com/browse/years/2008/page'+str(i)\n",
    "        \n",
    "        res_url_next_page = requests.get(url_next_page, headers=headers)         # URL specific get request\n",
    "        sf_soup_next_page = BeautifulSoup(res_url_next_page.content, 'lxml')     # URL specific soup object\n",
    "        \n",
    "        # Same scraping process as above\n",
    "        for i in range(39, 139):\n",
    "            song_and_artist = sf_soup_next_page.find_all('li')[i].text.replace('-', ',').split(',')\n",
    "            sf_songs_2008.append(song_and_artist[0])\n",
    "            sf_artists_2008.append(song_and_artist[1].replace(' ', ''))\n",
    "        \n",
    "        # 5 second break before rerunning the for loop\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The intuition behind the 2013 and 2018 scraping functions follows the same logic as the 2008 scraping function so commenting is excluded unless otherwise noted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_songs_2013 = []\n",
    "sf_artists_2013 = []\n",
    "\n",
    "def get_sf_songs_2013(url, song_list, artist_list):\n",
    "    res_url = requests.get(url, headers=headers)\n",
    "    sf_soup = BeautifulSoup(res_url.content, 'lxml')\n",
    "\n",
    "    for i in range(39, 139):\n",
    "        song_and_artist = sf_soup.find_all('li')[i].text.replace('-', ',').split(',')\n",
    "        sf_songs_2013.append(song_and_artist[0])\n",
    "        sf_artists_2013.append(song_and_artist[1].replace(' ', ''))\n",
    "            \n",
    "        break\n",
    "        \n",
    "    # New for loop to scrape songs and artists from page 2 through 29\n",
    "    # This range is specific for the year 2013      \n",
    "    for i in range(2,30):\n",
    "        url_next_page = 'https://www.songfacts.com/browse/years/2013/page'+str(i)\n",
    "        \n",
    "        res_url_next_page = requests.get(url_next_page, headers=headers)\n",
    "        sf_soup_next_page = BeautifulSoup(res_url_next_page.content, 'lxml')\n",
    "\n",
    "        for i in range(39, 139):\n",
    "            song_and_artist = sf_soup_next_page.find_all('li')[i].text.replace('-', ',').split(',')\n",
    "            sf_songs_2013.append(song_and_artist[0])\n",
    "            sf_artists_2013.append(song_and_artist[1].replace(' ', ''))\n",
    "            \n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_songs_2018 = []\n",
    "sf_artists_2018 = []\n",
    "\n",
    "def get_sf_songs_2018(url, song_list, artist_list):\n",
    "    res_url = requests.get(url, headers=headers)\n",
    "    sf_soup = BeautifulSoup(res_url.content, 'lxml')\n",
    "\n",
    "    for i in range(39, 139):\n",
    "        song_and_artist = sf_soup.find_all('li')[i].text.replace('-', ',').split(',')\n",
    "        sf_songs_2018.append(song_and_artist[0])\n",
    "        sf_artists_2018.append(song_and_artist[1].replace(' ', ''))\n",
    "            \n",
    "        break\n",
    "        \n",
    "    # New for loop to scrape songs and artists from page 2 through 15\n",
    "    # This range is specific for the year 2018     \n",
    "    for i in range(2,16):\n",
    "        url_next_page = 'https://www.songfacts.com/browse/years/2013/page'+str(i)\n",
    "        \n",
    "        res_url_next_page = requests.get(url_next_page, headers=headers)\n",
    "        sf_soup_next_page = BeautifulSoup(res_url_next_page.content, 'lxml')\n",
    "\n",
    "        for i in range(39, 139):\n",
    "            song_and_artist = sf_soup_next_page.find_all('li')[i].text.replace('-', ',').split(',')\n",
    "            sf_songs_2018.append(song_and_artist[0])\n",
    "            sf_artists_2018.append(song_and_artist[1].replace(' ', ''))\n",
    "            \n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sf_songs_2008(url_songfacts_2008, sf_songs_2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sf_songs_2013(url_songfacts_2013, sf_songs_2013)\n",
    "get_sf_songs_2018(url_songfacts_2018, sf_songs_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_songs_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_songs_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_url = 'https://genius.com/Drake-gods-plan-lyrics'\n",
    "lyr_res = requests.get(lyrics_url)\n",
    "lyr_soup = BeautifulSoup(lyr_res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr_soup.find_all('p')[0].text.replace('[Chorus 1]', '').replace('[Intro]', '').replace('[Post-Chorus]', '').replace('[Verse 1]', '').replace('[Verse 2]', '').replace('\\n', ' ').replace('[Chorus 2]', '').replace('[Bridge]', '').replace(\"'\", '').replace('\"', '').replace(',', '').replace('.', '').replace('?', '').replace('[Hook]', '').replace('[Verse 3]', '').replace('[Pre-Chorus]', '').replace('[Chorus]', '').replace('[Outro]', '').replace('(', '').replace(')', '').replace('-', ' ').replace('[Verse 4]', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's access Genius' API for song lyrics for all of our songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Genius object\n",
    "genius = genius.Genius(client_access_token='uThemSV0CHBXwge2qstA1ApXv_elTmbw8fABxs_GTfss_COk3MH_cwplfPenlryG', \n",
    "              response_format='plain',         # Format of response is plain text\n",
    "              timeout=5, sleep_time=0.5, \n",
    "              remove_section_headers=True,     # Remove headers such as [Intro], [Verse], [Chorus]n etc.\n",
    "              skip_non_songs=True,             # Skip items that are not songs\n",
    "              verbose=True)                    # Print search text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_name = ['thank u next', 'gods plan']\n",
    "artist_name = ['ariana grande', 'drake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(song_name, artist_name):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(song_name, artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "lyric_list_Hot100_2008 = []\n",
    "\n",
    "for song in get_track_names(url_2008):\n",
    "    song_2008 = genius.search_song(song).lyrics.replace('\\n', ' ').replace('.', '').replace(',', '').replace('-', ' ').replace(\"'\", '').replace('\"', '').replace('?', '')\n",
    "    lyric_list_Hot100_2008.append(song_2008)\n",
    "    time.sleep(3)\n",
    "    \n",
    "lyric_list_Hot100_2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_list_Hot100_2008[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = genius.search_song(\"God's Plan\").lyrics.replace('\\n', ' ').replace('.', '').replace(',', '').replace('-', ' ').replace(\"'\", '').replace('\"', '').replace('?', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's access Spotify's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up credentials and token for API environment\n",
    "credentials = oauth2.SpotifyClientCredentials(\n",
    "    client_id='24ae571f5509439a800b0bc9f45b9a3d',       # Client ID provided from developer account page\n",
    "    client_secret='de55987b16e54df0adbbf88074db3d57')   # Client Secret ID provided from developer account page\n",
    "                                                        # Client Secret ID has since been changed \n",
    "token = credentials.get_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spotify object\n",
    "spotify = spotipy.Spotify(auth=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have all of our hit songs, we need to get their URIs in order to extract the important audio features from Spotify's API. We set up separate `for` loops for each year's top tracks to get their individual URIs. We could have defined a function to extract all the URIs for all of our hit songs at once, but we thought it would be useful to have separate lists of URIs for each year's top tracks for our analysis because we will be comparing the change in features through the years.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to get URIs for 2018 hits\n",
    "billboard_tracks_URIs_2018 = []\n",
    "\n",
    "# Iterating through each key in the Top 2018 Tracks dictionary we created earlier\n",
    "for key in get_track_and_artist_names(url_2018).keys():\n",
    "    \n",
    "    # Spotify search query indexed to get the URI for each song\n",
    "    uri = spotify.search(track, limit=1, offset=0, type='track')['tracks']['items'][0]['uri'] \n",
    "    # Appending each URI to the empty list above\n",
    "    billboard_tracks_URIs_2018.append(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `for` loop above will be consistent for the other years we are examining.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to get URIs for 2013 hits\n",
    "billboard_tracks_URIs_2013 = []\n",
    "for key in get_track_and_artist_names(url_2013).keys():\n",
    "    uri = spotify.search(track, limit=1, offset=0, type='track')['tracks']['items'][0]['uri']\n",
    "    billboard_tracks_URIs_2013.append(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to get URIs for 2008 hits\n",
    "billboard_tracks_URIs_2008 = []\n",
    "for key in get_track_and_artist_names(url_2008).keys():\n",
    "    uri = spotify.search(track, limit=1, offset=0, type='track')['tracks']['items'][0]['uri']\n",
    "    billboard_tracks_URIs_2008.append(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SongFactsURIs = []\n",
    "for key in SF_track_and_artist_names.keys():\n",
    "    try:\n",
    "        uri = spotify.search(track, limit=1, offset=0, type='track')['tracks']['items'][0]['uri']\n",
    "    except:\n",
    "        pass\n",
    "    SongFactsURIs.append(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have the URIs for each song in the Year-End Hot 100 list for each year, we need to retrieve songs from each year that were not on this list. We conducted outside research to find songs that came out in each respective year not in the Hot 100 list and will retrieve their URIs through Spotify's API.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify.search(\"rockstar\", limit=1, offset=0, type='track')['tracks']['items'][0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = spotify.artist_albums(artist_id = 'spotify:artist:2P5sC9cVZDToPxyomzF1UH')\n",
    "artist['items'][2].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_URIs = []\n",
    "def artist_details(artist_URI):\n",
    "    artist = spotify.artist_albums(artist_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_URIs = []\n",
    "track_titles = []\n",
    "def album_details(album_URI):\n",
    "    album_songs = spotify.album_tracks(album_URI)\n",
    "    for i in range(album_songs['total']):\n",
    "        print(album_songs['items'][i]['name'])\n",
    "        track_titles.append(album_songs['items'][i]['name'])\n",
    "        track_URIs.append(album_songs['items'][i]['uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for i in range(len(trackURIs)):\n",
    "    print(i)\n",
    "    features.append(spotify.audio_features(trackURIs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features, index = trackNames[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally let's scrape Genius for song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
